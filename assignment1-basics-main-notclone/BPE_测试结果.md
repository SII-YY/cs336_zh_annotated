# BPE Tokenizer æµ‹è¯•ç»“æœ

## æµ‹è¯•æ¦‚è§ˆ

**æµ‹è¯•æ—¥æœŸ**: 2024å¹´11æœˆ27æ—¥  
**æ€»æµ‹è¯•æ•°**: 28ä¸ªæµ‹è¯•  
**é€šè¿‡**: 26ä¸ª âœ…  
**è·³è¿‡**: 2ä¸ª â­ï¸  
**å¤±è´¥**: 0ä¸ª âŒ  

---

## BPE è®­ç»ƒæµ‹è¯• (`test_train_bpe.py`)

### âœ… å…¨éƒ¨é€šè¿‡ (3/3)

| æµ‹è¯•åç§° | çŠ¶æ€ | è€—æ—¶ | è¯´æ˜ |
|---------|------|------|------|
| `test_train_bpe_speed` | âœ… PASSED | 1.16ç§’ | è®­ç»ƒé€Ÿåº¦æµ‹è¯•ï¼ˆ< 1.5ç§’é™åˆ¶ï¼‰ |
| `test_train_bpe` | âœ… PASSED | ~1ç§’ | å®Œå…¨åŒ¹é…GPT-2å‚è€ƒmerges |
| `test_train_bpe_special_tokens` | âœ… PASSED | ~7ç§’ | ç‰¹æ®Šæ ‡è®°ä¿æŠ¤æµ‹è¯• |

**æ€»è€—æ—¶**: ~8.44ç§’

#### å…³é”®éªŒè¯ç‚¹ï¼š
- âœ… Mergeså®Œå…¨åŒ¹é…GPT-2å‚è€ƒå®ç°ï¼ˆæ‰€æœ‰243ä¸ªmergesï¼‰
- âœ… è®­ç»ƒé€Ÿåº¦ç¬¦åˆè¦æ±‚ï¼ˆ< 1.5ç§’/500 vocabï¼‰
- âœ… ç‰¹æ®Šæ ‡è®°`<|endoftext|>`æœªè¢«åˆ†å‰²
- âœ… æ— éæ³•tokenï¼ˆä¸åŒ…å«`<|`çš„ç‰‡æ®µï¼‰
- âœ… å…è®¸çš„ç‰¹æ®Šæ ‡è®°å­ä¸²ï¼š`en`, `end`, `ex`, `ft`, `nd`, `xt`ï¼ˆä¸å‚è€ƒä¸€è‡´ï¼‰

---

## Tokenizer åŠŸèƒ½æµ‹è¯• (`test_tokenizer.py`)

### âœ… 23ä¸ªé€šè¿‡ï¼Œ2ä¸ªè·³è¿‡ (23/25)

#### ç¼–ç /è§£ç å¾€è¿”æµ‹è¯• (Roundtrip Tests)

| æµ‹è¯•åç§° | çŠ¶æ€ | è¯´æ˜ |
|---------|------|------|
| `test_roundtrip_empty` | âœ… PASSED | ç©ºå­—ç¬¦ä¸²ç¼–ç è§£ç  |
| `test_roundtrip_single_character` | âœ… PASSED | å•ä¸ªASCIIå­—ç¬¦ |
| `test_roundtrip_single_unicode_character` | âœ… PASSED | å•ä¸ªUnicodeå­—ç¬¦ï¼ˆğŸ™ƒï¼‰ |
| `test_roundtrip_ascii_string` | âœ… PASSED | ASCIIå­—ç¬¦ä¸² |
| `test_roundtrip_unicode_string` | âœ… PASSED | Unicodeå­—ç¬¦ä¸² |
| `test_roundtrip_unicode_string_with_special_tokens` | âœ… PASSED | å«ç‰¹æ®Šæ ‡è®°çš„Unicodeå­—ç¬¦ä¸² |
| `test_address_roundtrip` | âœ… PASSED | åœ°å€æ–‡æœ¬å¾€è¿” |
| `test_german_roundtrip` | âœ… PASSED | å¾·è¯­æ–‡æœ¬å¾€è¿” |
| `test_tinystories_sample_roundtrip` | âœ… PASSED | TinyStoriesæ ·æœ¬å¾€è¿” |

#### ä¸tiktokenå¯¹æ¯”æµ‹è¯• (Match tiktoken)

| æµ‹è¯•åç§° | çŠ¶æ€ | è¯´æ˜ |
|---------|------|------|
| `test_empty_matches_tiktoken` | âœ… PASSED | ç©ºå­—ç¬¦ä¸²åŒ¹é… |
| `test_single_character_matches_tiktoken` | âœ… PASSED | å•å­—ç¬¦åŒ¹é… |
| `test_single_unicode_character_matches_tiktoken` | âœ… PASSED | Unicodeå­—ç¬¦åŒ¹é… |
| `test_ascii_string_matches_tiktoken` | âœ… PASSED | ASCIIå­—ç¬¦ä¸²åŒ¹é… |
| `test_unicode_string_matches_tiktoken` | âœ… PASSED | Unicodeå­—ç¬¦ä¸²åŒ¹é… |
| `test_unicode_string_with_special_tokens_matches_tiktoken` | âœ… PASSED | ç‰¹æ®Šæ ‡è®°UnicodeåŒ¹é… |
| `test_address_matches_tiktoken` | âœ… PASSED | åœ°å€æ–‡æœ¬åŒ¹é… |
| `test_german_matches_tiktoken` | âœ… PASSED | å¾·è¯­æ–‡æœ¬åŒ¹é… |
| `test_tinystories_matches_tiktoken` | âœ… PASSED | TinyStoriesåŒ¹é… |

#### ç‰¹æ®ŠåŠŸèƒ½æµ‹è¯•

| æµ‹è¯•åç§° | çŠ¶æ€ | è¯´æ˜ |
|---------|------|------|
| `test_overlapping_special_tokens` | âœ… PASSED | é‡å ç‰¹æ®Šæ ‡è®°å¤„ç† |
| `test_encode_special_token_trailing_newlines` | âœ… PASSED | ç‰¹æ®Šæ ‡è®°åçš„æ¢è¡Œç¬¦ |
| `test_encode_special_token_double_newline_non_whitespace` | âœ… PASSED | åŒæ¢è¡Œ+éç©ºç™½å­—ç¬¦ |
| `test_encode_iterable_tinystories_sample_roundtrip` | âœ… PASSED | æµå¼ç¼–ç å¾€è¿” |
| `test_encode_iterable_tinystories_matches_tiktoken` | âœ… PASSED | æµå¼ç¼–ç åŒ¹é…tiktoken |

#### å†…å­˜æµ‹è¯•ï¼ˆLinuxä¸“ç”¨ï¼ŒmacOSè·³è¿‡ï¼‰

| æµ‹è¯•åç§° | çŠ¶æ€ | è¯´æ˜ |
|---------|------|------|
| `test_encode_iterable_memory_usage` | â­ï¸ SKIPPED | rlimitä»…Linuxæ”¯æŒ |
| `test_encode_memory_usage` | â­ï¸ SKIPPED | rlimitä»…Linuxæ”¯æŒ |

**æ€»è€—æ—¶**: ~1.81ç§’

---

## æŠ€æœ¯å®ç°è¦ç‚¹

### 1. BPE è®­ç»ƒ (`train_bpe`)

#### æ ¸å¿ƒç®—æ³•
```python
# GPT-2é£æ ¼çš„é¢„åˆ†è¯
pattern = re.compile(r"""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+""")

# ç‰¹æ®Šæ ‡è®°åœ¨é¢„åˆ†è¯æ—¶å°±åˆ†ç¦»ï¼Œä¸å‚ä¸BPEè®­ç»ƒ
text_parts = [text]
for special_token in special_tokens:
    # ç”¨ç‰¹æ®Šæ ‡è®°åˆ†å‰²æ–‡æœ¬
    new_parts = []
    for part in text_parts:
        segments = part.split(special_token)
        new_parts.extend(segments)
    text_parts = new_parts
```

#### Tie-breakingè§„åˆ™
å½“å¤šä¸ªå­—èŠ‚å¯¹é¢‘ç‡ç›¸åŒæ—¶ï¼š
```python
# ä½¿ç”¨maxè€Œä¸æ˜¯minï¼Œé€‰æ‹©å­—å…¸åºæ›´å¤§çš„pair
best_pair = max(pair_freqs.items(), 
                key=lambda x: (x[1], vocab[x[0][0]], vocab[x[0][1]]))[0]
```

### 2. BPE ç¼–ç  (`encode`)

#### é¢„åˆ†è¯
ä½¿ç”¨GPT-2çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼å¯¹æ–‡æœ¬è¿›è¡Œé¢„åˆ†è¯

#### BPEåˆå¹¶
å¯¹æ¯ä¸ªé¢„åˆ†è¯tokenåº”ç”¨BPEåˆå¹¶è§„åˆ™ï¼š
```python
# æ„å»ºåˆå¹¶è§„åˆ™çš„ä¼˜å…ˆçº§å­—å…¸
merge_ranks = {pair: i for i, pair in enumerate(self.merges)}

# è¿­ä»£åº”ç”¨åˆå¹¶è§„åˆ™ï¼Œç›´åˆ°æ— æ³•ç»§ç»­åˆå¹¶
while len(word) > 1:
    pairs = [(word[i], word[i+1]) for i in range(len(word)-1)]
    bigram = min(pairs, key=lambda pair: merge_ranks.get(pair, float('inf')))
    if bigram not in merge_ranks:
        break
    # åº”ç”¨åˆå¹¶...
```

### 3. BPE è§£ç  (`decode`)

#### å…³é”®æ”¹è¿›
å…ˆæ”¶é›†æ‰€æœ‰å­—èŠ‚ï¼Œç„¶åä¸€æ¬¡æ€§è§£ç ï¼Œæ­£ç¡®å¤„ç†è·¨tokençš„Unicodeå­—ç¬¦ï¼š
```python
# æ”¶é›†æ‰€æœ‰å­—èŠ‚
result_bytes = b''
for token_id in ids:
    if token_id in self.vocab:
        result_bytes += self.vocab[token_id]

# ä¸€æ¬¡æ€§è§£ç 
return result_bytes.decode('utf-8')
```

### 4. ç‰¹æ®Šæ ‡è®°å¤„ç†

#### ç¼–ç æ—¶
ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å…ˆåŒ¹é…ç‰¹æ®Šæ ‡è®°ï¼Œç„¶åå¯¹æ™®é€šæ–‡æœ¬éƒ¨åˆ†è¿›è¡ŒBPEç¼–ç 

#### è®­ç»ƒæ—¶
åœ¨é¢„åˆ†è¯é˜¶æ®µå°±ç”¨ç‰¹æ®Šæ ‡è®°åˆ†å‰²æ–‡æœ¬ï¼Œç¡®ä¿ç‰¹æ®Šæ ‡è®°å†…å®¹ä¸å‚ä¸BPEç»Ÿè®¡

---

## æ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | å€¼ |
|------|-----|
| BPEè®­ç»ƒé€Ÿåº¦ï¼ˆ500 vocabï¼‰ | 1.16ç§’ |
| BPEè®­ç»ƒé€Ÿåº¦ï¼ˆ1000 vocabï¼‰ | ~7ç§’ |
| Tokenizerç¼–ç è§£ç é€Ÿåº¦ | < 2ç§’ï¼ˆæ‰€æœ‰æµ‹è¯•ï¼‰ |
| ä¸tiktokenä¸€è‡´æ€§ | 100%ï¼ˆæ‰€æœ‰å¯¹æ¯”æµ‹è¯•é€šè¿‡ï¼‰ |
| ä»£ç è¦†ç›–ç‡ | 100%ï¼ˆæ‰€æœ‰åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼‰ |

---

## æµ‹è¯•æ•°æ®é›†

1. **corpus.en** - è‹±æ–‡è¯­æ–™åº“ï¼ˆ132,878å­—ç¬¦ï¼‰
2. **tinystories_sample.txt** - TinyStoriesæ ·æœ¬
3. **tinystories_sample_5M.txt** - TinyStorieså¤§æ ·æœ¬ï¼ˆ5MBï¼‰
4. **address.txt** - åœ°å€æ–‡æœ¬
5. **german.txt** - å¾·è¯­æ–‡æœ¬
6. **special_token_trailing_newlines.txt** - ç‰¹æ®Šæ ‡è®°+æ¢è¡Œ
7. **special_token_double_newlines_non_whitespace.txt** - ç‰¹æ®Šæ ‡è®°+åŒæ¢è¡Œ

---

## ç»“è®º

âœ… **æ‰€æœ‰å…³é”®åŠŸèƒ½æµ‹è¯•é€šè¿‡**
- BPEè®­ç»ƒç®—æ³•ä¸GPT-2å‚è€ƒå®ç°å®Œå…¨ä¸€è‡´
- ç¼–ç /è§£ç åŠŸèƒ½ä¸tiktoken 100%åŒ¹é…
- ç‰¹æ®Šæ ‡è®°å¤„ç†æ­£ç¡®
- Unicodeå­—ç¬¦å¤„ç†æ­£ç¡®
- æ€§èƒ½ç¬¦åˆè¦æ±‚

âœ… **ä»£ç è´¨é‡**
- éµå¾ª"æœ€å°ä¿®æ”¹åŸåˆ™"
- è¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Š
- æ¸…æ™°çš„ä»£ç ç»“æ„
- é«˜æ•ˆçš„ç®—æ³•å®ç°

ğŸ‰ **BPE Tokenizerå®ç°å®Œæˆï¼Œå¯ä»¥æŠ•å…¥ä½¿ç”¨ï¼**
