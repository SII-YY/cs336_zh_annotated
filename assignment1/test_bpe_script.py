#!/usr/bin/env python3  # æŒ‡å®šè„šæœ¬è§£é‡Šå™¨ä¸ºPython3
# -*- coding: utf-8 -*-  # æŒ‡å®šæ–‡ä»¶ç¼–ç ä¸ºUTF-8ï¼Œæ”¯æŒä¸­æ–‡

"""
# BPEåˆ†è¯å™¨æµ‹è¯•è„šæœ¬
# 
# è¿™ä¸ªè„šæœ¬æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ä¸åŒçš„æµ‹è¯•æ–‡ä»¶è®­ç»ƒBPEåˆ†è¯å™¨ï¼Œå¹¶å±•ç¤ºè®­ç»ƒç»“æœã€‚
# æ”¯æŒå¤šç§æµ‹è¯•æ–‡ä»¶é€‰æ‹©ã€è‡ªå®šä¹‰è¯æ±‡è¡¨å¤§å°å’Œç»“æœå¯è§†åŒ–å±•ç¤ºã€‚
"""

import os  # å¯¼å…¥osæ¨¡å—ï¼Œç”¨äºæ–‡ä»¶è·¯å¾„æ“ä½œ
import sys  # å¯¼å…¥sysæ¨¡å—ï¼Œç”¨äºç³»ç»Ÿç›¸å…³æ“ä½œ
from typing import List, Tuple, Dict, Any  # å¯¼å…¥ç±»å‹æ³¨è§£ï¼Œç”¨äºå‡½æ•°å‚æ•°å’Œè¿”å›å€¼ç±»å‹æ ‡æ³¨

# å¯¼å…¥BPEåˆ†è¯å™¨æ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))  # å°†çˆ¶ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥å…¶ä»–æ¨¡å—

# æµ‹è¯•æ–‡ä»¶è·¯å¾„
test_files = {  # å®šä¹‰æµ‹è¯•æ–‡ä»¶å­—å…¸ï¼Œé”®ä¸ºé€‰æ‹©åºå·ï¼Œå€¼ä¸ºæ–‡ä»¶å
    1: "test_english.txt",  # çº¯è‹±æ–‡æµ‹è¯•æ–‡ä»¶
    2: "test_chinese_english.txt",  # ä¸­è‹±æ–‡æ··åˆæµ‹è¯•æ–‡ä»¶
    3: "test_special_chars.txt"  # ç‰¹æ®Šå­—ç¬¦æµ‹è¯•æ–‡ä»¶
}

# é»˜è®¤å‚æ•°
DEFAULT_VOCAB_SIZE = 1000  # é»˜è®¤è¯æ±‡è¡¨å¤§å°
DEFAULT_SPECIAL_TOKENS = ["<UNK>", "<PAD>", "<SOS>", "<EOS>"]  # é»˜è®¤ç‰¹æ®Šæ ‡è®°åˆ—è¡¨
# <UNK>: æœªçŸ¥æ ‡è®°, <PAD>: å¡«å……æ ‡è®°, <SOS>: åºåˆ—å¼€å§‹æ ‡è®°, <EOS>: åºåˆ—ç»“æŸæ ‡è®°


def print_separator():  # å®šä¹‰æ‰“å°åˆ†éš”çº¿çš„å‡½æ•°
    """æ‰“å°åˆ†éš”çº¿"""
    print("=" * 70)  # æ‰“å°70ä¸ªç­‰å·ä½œä¸ºåˆ†éš”çº¿


def select_test_file() -> str:  # å®šä¹‰é€‰æ‹©æµ‹è¯•æ–‡ä»¶çš„å‡½æ•°ï¼Œè¿”å›å­—ç¬¦ä¸²ç±»å‹çš„æ–‡ä»¶è·¯å¾„
    """è®©ç”¨æˆ·é€‰æ‹©æµ‹è¯•æ–‡ä»¶"""
    print("è¯·é€‰æ‹©æµ‹è¯•æ–‡ä»¶ï¼š")  # æ‰“å°æç¤ºä¿¡æ¯
    for idx, file_name in test_files.items():  # éå†æµ‹è¯•æ–‡ä»¶å­—å…¸
        print(f"{idx}. {file_name}")  # æ‰“å°æ¯ä¸ªæ–‡ä»¶çš„é€‰æ‹©åºå·å’Œæ–‡ä»¶å
    
    while True:  # æ— é™å¾ªç¯ï¼Œç›´åˆ°ç”¨æˆ·è¾“å…¥æœ‰æ•ˆé€‰é¡¹
        try:  # å¼‚å¸¸å¤„ç†å—
            choice = int(input("è¯·è¾“å…¥é€‰æ‹© (1-3): "))  # è·å–ç”¨æˆ·è¾“å…¥çš„æ•´æ•°é€‰æ‹©
            if choice in test_files:  # æ£€æŸ¥é€‰æ‹©æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
                # æ„å»ºå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
                file_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), test_files[choice])
                if os.path.exists(file_path):  # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                    return file_path  # è¿”å›æ–‡ä»¶è·¯å¾„
                else:  # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨
                    print(f"é”™è¯¯ï¼šæ–‡ä»¶ {test_files[choice]} ä¸å­˜åœ¨ï¼")  # æ‰“å°é”™è¯¯ä¿¡æ¯
            else:  # å¦‚æœé€‰æ‹©ä¸åœ¨æœ‰æ•ˆèŒƒå›´å†…
                print("é”™è¯¯ï¼šè¯·è¾“å…¥æœ‰æ•ˆçš„é€‰é¡¹ (1-3)")  # æ‰“å°é”™è¯¯ä¿¡æ¯
        except ValueError:  # æ•è·å€¼é”™è¯¯å¼‚å¸¸ï¼ˆéæ•´æ•°è¾“å…¥ï¼‰
            print("é”™è¯¯ï¼šè¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")  # æ‰“å°é”™è¯¯ä¿¡æ¯


def get_vocab_size() -> int:  # å®šä¹‰è·å–è¯æ±‡è¡¨å¤§å°çš„å‡½æ•°ï¼Œè¿”å›æ•´æ•°ç±»å‹
    """è·å–ç”¨æˆ·æŒ‡å®šçš„è¯æ±‡è¡¨å¤§å°"""
    while True:  # æ— é™å¾ªç¯ï¼Œç›´åˆ°ç”¨æˆ·è¾“å…¥æœ‰æ•ˆå¤§å°
        try:  # å¼‚å¸¸å¤„ç†å—
            # è·å–ç”¨æˆ·è¾“å…¥ï¼Œæç¤ºé»˜è®¤å€¼
            size = input(f"è¯·è¾“å…¥è¯æ±‡è¡¨å¤§å° (é»˜è®¤: {DEFAULT_VOCAB_SIZE}): ")
            if size.strip() == "":  # å¦‚æœç”¨æˆ·ç›´æ¥æŒ‰å›è½¦ï¼ˆç©ºè¾“å…¥ï¼‰
                return DEFAULT_VOCAB_SIZE  # è¿”å›é»˜è®¤è¯æ±‡è¡¨å¤§å°
            vocab_size = int(size)  # å°è¯•å°†è¾“å…¥è½¬æ¢ä¸ºæ•´æ•°
            if vocab_size > 256:  # æ£€æŸ¥è¯æ±‡è¡¨å¤§å°æ˜¯å¦å¤§äº256ï¼ˆæ‰€æœ‰å¯èƒ½çš„å­—èŠ‚æ•°é‡ï¼‰
                return vocab_size  # è¿”å›ç”¨æˆ·æŒ‡å®šçš„è¯æ±‡è¡¨å¤§å°
            else:  # å¦‚æœè¯æ±‡è¡¨å¤§å°è¿‡å°
                print("é”™è¯¯ï¼šè¯æ±‡è¡¨å¤§å°å¿…é¡»å¤§äº256")  # æ‰“å°é”™è¯¯ä¿¡æ¯
        except ValueError:  # æ•è·å€¼é”™è¯¯å¼‚å¸¸ï¼ˆéæ•´æ•°è¾“å…¥ï¼‰
            print("é”™è¯¯ï¼šè¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")  # æ‰“å°é”™è¯¯ä¿¡æ¯


def show_training_results(vocab: Dict[int, bytes], merges: List[Tuple[bytes, bytes]]):  # å®šä¹‰å±•ç¤ºè®­ç»ƒç»“æœçš„å‡½æ•°
    # å‚æ•°ï¼švocab - è¯æ±‡è¡¨å­—å…¸ï¼Œé”®ä¸ºtoken IDï¼Œå€¼ä¸ºå­—èŠ‚åºåˆ—ï¼›merges - åˆå¹¶è§„åˆ™åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªå­—èŠ‚åºåˆ—çš„å…ƒç»„
    """å±•ç¤ºè®­ç»ƒç»“æœ"""
    print_separator()  # æ‰“å°åˆ†éš”çº¿
    print(f"ğŸ¯ è®­ç»ƒå®Œæˆï¼")  # æ‰“å°å®Œæˆæç¤º
    print(f"ğŸ“Š è¯æ±‡è¡¨å¤§å°: {len(vocab)}")  # æ‰“å°è¯æ±‡è¡¨å¤§å°
    print(f"ğŸ”„ åˆå¹¶æ“ä½œæ•°é‡: {len(merges)}")  # æ‰“å°åˆå¹¶æ“ä½œæ•°é‡
    print_separator()  # æ‰“å°åˆ†éš”çº¿
    
    # æ˜¾ç¤ºéƒ¨åˆ†è¯æ±‡è¡¨å†…å®¹
    print("ğŸ“ è¯æ±‡è¡¨æ ·ä¾‹ (å‰10ä¸ªå’Œå10ä¸ª):")  # æ‰“å°æç¤º
    # æ˜¾ç¤ºå‰10ä¸ªè¯æ±‡
    print("å‰10ä¸ªè¯æ±‡:")  # æ‰“å°æç¤º
    for i, (token_id, token_bytes) in enumerate(list(vocab.items())[:10]):  # éå†å‰10ä¸ªè¯æ±‡é¡¹
        try:  # å¼‚å¸¸å¤„ç†å—
            # å°è¯•è§£ç æ˜¾ç¤ºï¼Œä½¿ç”¨errors='replace'æ›¿æ¢æ— æ³•è§£ç çš„å­—ç¬¦
            token_str = token_bytes.decode('utf-8', errors='replace')
            # æ‰“å°token IDã€åŸå§‹å­—èŠ‚è¡¨ç¤ºå’Œè§£ç åçš„å­—ç¬¦ä¸²è¡¨ç¤º
            print(f"  {token_id}: {token_bytes!r} -> {token_str!r}")
        except Exception as e:  # æ•è·ä»»ä½•å¼‚å¸¸
            # æ‰“å°è§£ç é”™è¯¯ä¿¡æ¯
            print(f"  {token_id}: {token_bytes!r} -> è§£ç é”™è¯¯")
    
    # æ˜¾ç¤ºå10ä¸ªè¯æ±‡
    print("å10ä¸ªè¯æ±‡:")  # æ‰“å°æç¤º
    for i, (token_id, token_bytes) in enumerate(list(vocab.items())[-10:]):  # éå†å10ä¸ªè¯æ±‡é¡¹
        try:  # å¼‚å¸¸å¤„ç†å—
            # å°è¯•è§£ç æ˜¾ç¤º
            token_str = token_bytes.decode('utf-8', errors='replace')
            # æ‰“å°token IDã€åŸå§‹å­—èŠ‚è¡¨ç¤ºå’Œè§£ç åçš„å­—ç¬¦ä¸²è¡¨ç¤º
            print(f"  {token_id}: {token_bytes!r} -> {token_str!r}")
        except Exception as e:  # æ•è·ä»»ä½•å¼‚å¸¸
            # æ‰“å°è§£ç é”™è¯¯ä¿¡æ¯
            print(f"  {token_id}: {token_bytes!r} -> è§£ç é”™è¯¯")
    
    # æ˜¾ç¤ºéƒ¨åˆ†åˆå¹¶è§„åˆ™
    print_separator()  # æ‰“å°åˆ†éš”çº¿
    if merges:  # å¦‚æœå­˜åœ¨åˆå¹¶è§„åˆ™
        print(f"ğŸ”— åˆå¹¶è§„åˆ™æ ·ä¾‹ (å‰5ä¸ª):")  # æ‰“å°æç¤º
        for i, (pair1, pair2) in enumerate(merges[:5]):  # éå†å‰5ä¸ªåˆå¹¶è§„åˆ™
            try:  # å¼‚å¸¸å¤„ç†å—
                # å°è¯•è§£ç æ˜¾ç¤ºåˆå¹¶å¯¹
                pair1_str = pair1.decode('utf-8', errors='replace')
                pair2_str = pair2.decode('utf-8', errors='replace')
                # æ‰“å°åˆå¹¶è§„åˆ™ï¼ŒåŒ…æ‹¬åŸå§‹å­—èŠ‚è¡¨ç¤ºå’Œè§£ç åçš„å­—ç¬¦ä¸²è¡¨ç¤º
                print(f"  {i+1}. {pair1!r}({pair1_str}) + {pair2!r}({pair2_str}) -> {pair1+pair2!r}")
            except Exception:  # æ•è·ä»»ä½•å¼‚å¸¸
                # æ‰“å°æ— æ³•è§£ç çš„åˆå¹¶è§„åˆ™
                print(f"  {i+1}. {pair1!r} + {pair2!r} -> {pair1+pair2!r}")
    else:  # å¦‚æœä¸å­˜åœ¨åˆå¹¶è§„åˆ™
        print("ğŸ”— æ²¡æœ‰ç”Ÿæˆåˆå¹¶è§„åˆ™")  # æ‰“å°æç¤º


def test_tokenization():  # å®šä¹‰æµ‹è¯•åˆ†è¯åŠŸèƒ½çš„å‡½æ•°
    """æµ‹è¯•åˆ†è¯åŠŸèƒ½"""
    # æ³¨æ„ï¼šè¿™ä¸ªå‡½æ•°åªæ˜¯ä¸€ä¸ªå ä½ç¬¦ï¼Œå®é™…çš„åˆ†è¯æµ‹è¯•éœ€è¦æ ¹æ®assignment1_zh_annotated_from_scratch.pyä¸­çš„å…·ä½“å®ç°æ¥è°ƒæ•´
    print_separator()  # æ‰“å°åˆ†éš”çº¿
    print("ğŸ” åˆ†è¯æµ‹è¯•åŠŸèƒ½å°†åœ¨æ‚¨æ­£ç¡®å®ç°get_tokenizerå‡½æ•°åå¯ç”¨")  # æ‰“å°æç¤ºä¿¡æ¯
    print("è¯·æ ¹æ®assignment1_zh_annotated_from_scratch.pyä¸­çš„å®ç°æ¥æ‰©å±•è¿™ä¸ªå‡½æ•°")  # æ‰“å°æç¤ºä¿¡æ¯


def main():  # å®šä¹‰ä¸»å‡½æ•°
    """ä¸»å‡½æ•°"""
    print_separator()  # æ‰“å°åˆ†éš”çº¿
    print("ğŸš€ BPEåˆ†è¯å™¨æµ‹è¯•è„šæœ¬")  # æ‰“å°è„šæœ¬æ ‡é¢˜
    print("è¿™ä¸ªè„šæœ¬ç”¨äºæµ‹è¯•assignment1_zh_annotated_from_scratch.pyä¸­çš„BPEåˆ†è¯å™¨åŠŸèƒ½")  # æ‰“å°è„šæœ¬è¯´æ˜
    print_separator()  # æ‰“å°åˆ†éš”çº¿
    
    # åŠ¨æ€å¯¼å…¥BPEåˆ†è¯å™¨å‡½æ•°
    try:  # å¼‚å¸¸å¤„ç†å—
        # å°è¯•å¯¼å…¥å¿…è¦çš„å‡½æ•°
        from assignment1_zh_annotated_from_scratch import train_bpe_tokenizer, run_train_bpe, get_tokenizer
        print("âœ… æˆåŠŸå¯¼å…¥BPEåˆ†è¯å™¨æ¨¡å—")  # æ‰“å°æˆåŠŸä¿¡æ¯
    except ImportError as e:  # æ•è·å¯¼å…¥é”™è¯¯å¼‚å¸¸
        print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")  # æ‰“å°é”™è¯¯ä¿¡æ¯
        print("è¯·ç¡®ä¿assignment1_zh_annotated_from_scratch.pyæ–‡ä»¶å­˜åœ¨ä¸”å®ç°äº†å¿…è¦çš„å‡½æ•°")  # æ‰“å°æç¤º
        sys.exit(1)  # é€€å‡ºç¨‹åºï¼Œè¿”å›é”™è¯¯ç 1
    
    # é€‰æ‹©æµ‹è¯•æ–‡ä»¶
    input_path = select_test_file()  # è°ƒç”¨å‡½æ•°è·å–ç”¨æˆ·é€‰æ‹©çš„æµ‹è¯•æ–‡ä»¶è·¯å¾„
    print(f"ğŸ“„ é€‰æ‹©çš„æµ‹è¯•æ–‡ä»¶: {os.path.basename(input_path)}")  # æ‰“å°é€‰æ‹©çš„æ–‡ä»¶å
    
    # è·å–è¯æ±‡è¡¨å¤§å°
    vocab_size = get_vocab_size()  # è°ƒç”¨å‡½æ•°è·å–ç”¨æˆ·æŒ‡å®šçš„è¯æ±‡è¡¨å¤§å°
    print(f"ğŸ“ è¯æ±‡è¡¨å¤§å°: {vocab_size}")  # æ‰“å°è¯æ±‡è¡¨å¤§å°
    
    # ä½¿ç”¨é»˜è®¤ç‰¹æ®Štoken
    special_tokens = DEFAULT_SPECIAL_TOKENS  # ä½¿ç”¨é¢„å®šä¹‰çš„ç‰¹æ®Šæ ‡è®°åˆ—è¡¨
    print(f"ğŸ”¤ ç‰¹æ®Štoken: {special_tokens}")  # æ‰“å°ç‰¹æ®Šæ ‡è®°åˆ—è¡¨
    
    # å¼€å§‹è®­ç»ƒ
    print_separator()  # æ‰“å°åˆ†éš”çº¿
    print(f"ğŸƒâ€â™‚ï¸ å¼€å§‹è®­ç»ƒBPEåˆ†è¯å™¨...")  # æ‰“å°å¼€å§‹è®­ç»ƒæç¤º
    try:  # å¼‚å¸¸å¤„ç†å—
        # è°ƒç”¨è®­ç»ƒå‡½æ•°ï¼Œä¼ å…¥æ–‡ä»¶è·¯å¾„ã€è¯æ±‡è¡¨å¤§å°å’Œç‰¹æ®Šæ ‡è®°åˆ—è¡¨
        vocab, merges = train_bpe_tokenizer(input_path, vocab_size, special_tokens)
        
        # æ˜¾ç¤ºè®­ç»ƒç»“æœ
        show_training_results(vocab, merges)  # è°ƒç”¨å‡½æ•°å±•ç¤ºè®­ç»ƒç»“æœ
        
        # ç®€å•æµ‹è¯•run_train_bpeå‡½æ•°
        try:  # åµŒå¥—å¼‚å¸¸å¤„ç†å—
            print_separator()  # æ‰“å°åˆ†éš”çº¿
            print("ğŸ§ª æµ‹è¯•run_train_bpeå‡½æ•°...")  # æ‰“å°æµ‹è¯•æç¤º
            # è°ƒç”¨run_train_bpeå‡½æ•°
            vocab2, merges2 = run_train_bpe(input_path, vocab_size, special_tokens)
            print(f"âœ… run_train_bpeå‡½æ•°è°ƒç”¨æˆåŠŸï¼Œè¿”å›ç±»å‹æ­£ç¡®")  # æ‰“å°æˆåŠŸä¿¡æ¯
        except Exception as e:  # æ•è·ä»»ä½•å¼‚å¸¸
            print(f"âŒ run_train_bpeå‡½æ•°è°ƒç”¨å‡ºé”™: {e}")  # æ‰“å°é”™è¯¯ä¿¡æ¯
        
        # æç¤ºç”¨æˆ·å¯ä»¥è¿›è¡Œåˆ†è¯æµ‹è¯•
        test_tokenization()  # è°ƒç”¨åˆ†è¯æµ‹è¯•å‡½æ•°
        
    except Exception as e:  # æ•è·è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä»»ä½•å¼‚å¸¸
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")  # æ‰“å°é”™è¯¯ä¿¡æ¯
        import traceback  # å¯¼å…¥tracebackæ¨¡å—ï¼Œç”¨äºæ‰“å°è¯¦ç»†çš„é”™è¯¯å †æ ˆ
        traceback.print_exc()  # æ‰“å°è¯¦ç»†çš„é”™è¯¯å †æ ˆä¿¡æ¯
    
    print_separator()  # æ‰“å°åˆ†éš”çº¿
    print("ğŸ‘‹ æµ‹è¯•å®Œæˆï¼")  # æ‰“å°å®Œæˆæç¤º


if __name__ == "__main__":  # æ£€æŸ¥æ˜¯å¦ç›´æ¥è¿è¡Œè„šæœ¬ï¼ˆéå¯¼å…¥ï¼‰
    main()  # è°ƒç”¨ä¸»å‡½æ•°
